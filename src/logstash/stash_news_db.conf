# Logstash를 사용하여 PostgreSQL에서 뉴스 데이터를 가져와 Elasticsearch에 저장하는 구성
# 이 스크립트는 Logstash의 JDBC 플러그인을 사용하여 PostgreSQL에서 뉴스 데이터를 읽고,
# 필요한 필드만 선택적으로 Elasticsearch로 전송하는 설정입니다.

# 데이터 입력 단계: PostgreSQL에서 뉴스 데이터를 가져옵니다.
input {
  # JDBC 플러그인 설정
  jdbc {
    # JDBC 드라이버 라이브러리 위치 (PostgreSQL 드라이버 필요)
    jdbc_driver_library => "/usr/share/logstash/ingest_data/postgresql.jar"
    # 사용할 JDBC 드라이버 클래스
    jdbc_driver_class => "org.postgresql.Driver"
    # PostgreSQL 연결 문자열 (컨테이너 환경에서 내부 네트워크를 사용)
    jdbc_connection_string => "jdbc:postgresql://postgres:5432/news"
    # 데이터베이스 사용자명 및 비밀번호 (직접 지정)
    jdbc_user => "juyeon1"
    jdbc_password => "juyeon1"
    # 데이터 가져오기 주기 (크론 표현식 사용 - 매 1분마다 실행)
    schedule => "* * * * *"
    # 실행할 SQL 쿼리 (news_article 테이블에서 필요한 필드만 조회)
    statement => "SELECT id, title, content, category, CAST(keywords AS TEXT) AS keywords FROM news_article"
  }
}

# 데이터 변환 단계: JSON 형식의 키워드 처리 및 불필요한 필드 제거
filter {
  # JSON 필드 파싱 - keywords는 PostgreSQL에서 JSON 형식으로 저장됨
  json {
    source => "keywords"
    target => "keywords"
  }

  # Elasticsearch에 저장 시 불필요한 Logstash 기본 필드 제거
  mutate {
    remove_field => ["@timestamp", "@version"]
  }
}

# 데이터 출력 단계: Elasticsearch로 전송합니다.
output {
  # Elasticsearch로 데이터 전송
  elasticsearch {
    # Elasticsearch의 호스트 주소 (기본 포트 9200)
    hosts => ["http://es01:9200"]
    # 저장할 인덱스 이름
    index => "news"
    # document_id를 설정하여 동일한 ID의 데이터가 덮어쓰이도록 설정
    document_id => "%{id}"
  }
  
  # 표준 출력 (stdout)에서 데이터를 확인할 수 있도록 설정
  stdout { codec => rubydebug }
}

# 참고 문서:
# Logstash JDBC Input Plugin: https://www.elastic.co/guide/en/logstash/current/plugins-inputs-jdbc.html
# Logstash Mutate Filter: https://www.elastic.co/guide/en/logstash/current/plugins-filters-mutate.html
# Logstash Elasticsearch Output Plugin: https://www.elastic.co/guide/en/logstash/current/plugins-outputs-elasticsearch.html